{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPJuO5ItFMEUtY16ANzaZwR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JiZ6sAW98nnD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQa-m5-YY1b8"
      },
      "outputs": [],
      "source": [
        "# T5 Text-to-SQL Evaluation Script\n",
        "# Generates predictions and prepares files for local evaluation.\n",
        "# VERSION: Aligned with T5-Large training (No fp16)\n",
        "# Includes fix for missing tokenizer files and points to checkpoint.\n",
        "\n",
        "# First, mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # Added force_remount=True just in case\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Show GPU info\n",
        "!nvidia-smi\n",
        "\n",
        "# Install required packages\n",
        "print(\"Installing required packages (matching training environment)...\")\n",
        "!pip install -q datasets transformers evaluate tensorboard accelerate huggingface-hub pandas\n",
        "print(\"Packages installed.\")\n",
        "\n",
        "# --- Verify Installation ---\n",
        "print(\"\\nVerifying package versions...\")\n",
        "!pip show datasets transformers evaluate tensorboard accelerate huggingface-hub torch pandas\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import subprocess\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/text2sql\"\n",
        "EXPERIMENT_NAME = \"t5_large_sql_types_schema_v5\"\n",
        "SCHEMA_FORMAT = \"sql\"\n",
        "LOCAL_DATASET_DIR = \"/content/datasets/spider\"\n",
        "MODEL_PATH = f\"{DRIVE_BASE_DIR}/{EXPERIMENT_NAME}\"\n",
        "OUTPUT_DIR = f\"{DRIVE_BASE_DIR}/eval_results/{EXPERIMENT_NAME}_best\"\n",
        "NUM_BEAMS = 8\n",
        "LIMIT = 0\n",
        "RUN_SPIDER_EVAL = False\n",
        "SPIDER_EVAL_SCRIPT_PATH = f\"{DRIVE_BASE_DIR}/spider_evaluation/evaluation.py\"\n",
        "LOCAL_DB_DIR = f\"{LOCAL_DATASET_DIR}/database\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"--- Evaluation Configuration ---\")\n",
        "print(f\"Model path: {MODEL_PATH}\")\n",
        "print(f\"Schema format: {SCHEMA_FORMAT} (Expecting Types)\")\n",
        "print(f\"Output directory (in Google Drive): {OUTPUT_DIR}\")\n",
        "print(f\"Local Colab Dataset directory: {LOCAL_DATASET_DIR}\")\n",
        "print(f\"Num beams: {NUM_BEAMS}\")\n",
        "print(f\"Limit examples: {'All' if LIMIT <= 0 else LIMIT}\")\n",
        "print(f\"Run Spider eval script automatically in Colab: {RUN_SPIDER_EVAL}\")\n",
        "if not RUN_SPIDER_EVAL:\n",
        "     print(f\" -> Files for local evaluation will be saved to: {OUTPUT_DIR}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Schema Utilities ---\n",
        "if not os.path.exists('schema_utils.py'):\n",
        "    print(\"schema_utils.py not found, writing from string...\")\n",
        "    schema_utils_content = \"\"\"\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Use LOCAL_DATASET_DIR which should be populated by setup_local_dataset_from_drive\n",
        "LOCAL_DATASET_DIR_SCHEMA = \"/content/datasets/spider\"\n",
        "\n",
        "def load_tables_json(tables_path: str = 'tables.json') -> Dict[str, Any]:\n",
        "    \\\"\\\"\\\"Load the tables.json file containing schema information for all databases.\\\"\\\"\\\"\n",
        "    actual_path = os.path.join(LOCAL_DATASET_DIR_SCHEMA, tables_path)\n",
        "    print(f\"Schema Utils: Loading tables.json from: {actual_path}\")\n",
        "    try:\n",
        "        with open(actual_path, 'r', encoding='utf-8') as f:\n",
        "            tables_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"CRITICAL Error in schema_utils: tables.json not found at {actual_path}\")\n",
        "        raise\n",
        "    db_schemas = {}\n",
        "    for db_info in tables_data:\n",
        "        db_id = db_info['db_id']\n",
        "        db_schemas[db_id] = db_info\n",
        "    return db_schemas\n",
        "\n",
        "def get_sql_schema_string(db_id: str, db_schemas: Dict[str, Any]) -> str:\n",
        "    \\\"\\\"\\\"Create SQL schema string including types, PKs, FKs.\\\"\\\"\\\"\n",
        "    if db_id not in db_schemas: raise ValueError(f\"DB ID '{db_id}' not found\")\n",
        "    schema_info = db_schemas[db_id]\n",
        "    tables = schema_info['table_names_original']\n",
        "    columns = schema_info['column_names_original']\n",
        "    column_types = schema_info['column_types']\n",
        "    primary_keys = set(schema_info.get('primary_keys', []))\n",
        "    fk_dict = {}\n",
        "    if isinstance(schema_info.get('foreign_keys'), list):\n",
        "        for fk_pair in schema_info['foreign_keys']:\n",
        "             if isinstance(fk_pair, (list, tuple)) and len(fk_pair) == 2:\n",
        "                 col1_idx, col2_idx = fk_pair\n",
        "                 if isinstance(col1_idx, int) and isinstance(col2_idx, int): fk_dict[col1_idx] = col2_idx\n",
        "    table_defs = []\n",
        "    for i, table in enumerate(tables):\n",
        "        table_columns = []\n",
        "        for col_idx, (tab_idx, col_name) in enumerate(columns):\n",
        "            if tab_idx == i:\n",
        "                col_type = column_types[col_idx].upper()\n",
        "                col_info = f\"{col_name} ({col_type})\"\n",
        "                if col_idx in primary_keys: col_info += \" (PRIMARY KEY)\"\n",
        "                if col_idx in fk_dict:\n",
        "                    ref_col_idx = fk_dict[col_idx]\n",
        "                    if 0 <= ref_col_idx < len(columns):\n",
        "                         ref_tab_idx, ref_col_name = columns[ref_col_idx]\n",
        "                         if 0 <= ref_tab_idx < len(tables):\n",
        "                              ref_table = tables[ref_tab_idx]\n",
        "                              col_info += f\" (FOREIGN KEY -> {ref_table}.{ref_col_name})\"\n",
        "                table_columns.append(col_info)\n",
        "        if table_columns:\n",
        "            table_columns.sort()\n",
        "            table_def = f\"Table: {table}\\\\nColumns: {', '.join(table_columns)}\"\n",
        "            table_defs.append(table_def)\n",
        "    table_defs.sort()\n",
        "    return \"\\\\n\".join(table_defs)\n",
        "\n",
        "def get_compact_schema_string(db_id: str, db_schemas: Dict[str, Any]) -> str:\n",
        "    \\\"\\\"\\\"Create compact schema string.\\\"\\\"\\\"\n",
        "    if db_id not in db_schemas: raise ValueError(f\"DB ID '{db_id}' not found\")\n",
        "    schema_info = db_schemas[db_id]\n",
        "    tables = schema_info['table_names_original']\n",
        "    columns = schema_info['column_names_original']\n",
        "    table_columns = {}\n",
        "    for i, table in enumerate(tables):\n",
        "        cols = []\n",
        "        for tab_idx, col_name in columns:\n",
        "            if tab_idx == i: cols.append(col_name)\n",
        "        if cols:\n",
        "             cols.sort()\n",
        "             table_columns[table] = cols\n",
        "    parts = []\n",
        "    for table in sorted(table_columns.keys()):\n",
        "        cols = table_columns[table]\n",
        "        part = f\"{table}({', '.join(cols)})\"\n",
        "        parts.append(part)\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def enhance_prompts_with_schema(data_df: pd.DataFrame, db_schemas: Dict[str, Any], schema_format: str = \"sql\") -> pd.DataFrame:\n",
        "    \\\"\\\"\\\"Enhance input prompts with schema information.\\\"\\\"\\\"\n",
        "    enhanced_rows = []\n",
        "    skipped_count = 0\n",
        "    total_count = len(data_df)\n",
        "    print_interval = max(1, total_count // 10)\n",
        "    print(f\"Enhancing {total_count} prompts...\")\n",
        "    for index, example in data_df.iterrows():\n",
        "        if index > 0 and index % print_interval == 0: print(f\"  Processed {index}/{total_count} examples...\")\n",
        "        db_id = example['db_id']\n",
        "        try:\n",
        "            if schema_format == \"compact\":\n",
        "                schema_str = get_compact_schema_string(db_id, db_schemas)\n",
        "                input_text = f\"translate English to SQL: {example['question']} | database: {db_id} | schema: {schema_str}\"\n",
        "            elif schema_format == \"sql\":\n",
        "                schema_str = get_sql_schema_string(db_id, db_schemas) # Uses func with types\n",
        "                input_text = f\"translate English to SQL: {example['question']} | database: {db_id} | schema:\\\\n{schema_str}\"\n",
        "            elif schema_format == \"both\":\n",
        "                compact_str = get_compact_schema_string(db_id, db_schemas)\n",
        "                sql_str = get_sql_schema_string(db_id, db_schemas) # Uses func with types\n",
        "                input_text = f\"translate English to SQL: {example['question']} | database: {db_id} | schema: {compact_str}\\\\nDetailed schema:\\\\n{sql_str}\"\n",
        "            else: raise ValueError(f\"Unknown schema format: {schema_format}\")\n",
        "            output_text = example['query']\n",
        "            enhanced_rows.append({\"input_text\": input_text, \"output_text\": output_text})\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Skipping example for db_id '{db_id}' due to error: {e}\")\n",
        "             skipped_count += 1\n",
        "    print(f\"  Processed {total_count}/{total_count} examples...\")\n",
        "    if skipped_count > 0: print(f\"Skipped {skipped_count} examples.\")\n",
        "    else: print(\"Successfully enhanced all prompts.\")\n",
        "    if not enhanced_rows: print(\"Warning: No rows enhanced.\")\n",
        "    return pd.DataFrame(enhanced_rows)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open('schema_utils.py', 'w', encoding='utf-8') as f:\n",
        "            f.write(schema_utils_content.strip())\n",
        "        print(\"Successfully wrote schema_utils.py\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing schema_utils.py: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Function to setup dataset ---\n",
        "def setup_local_dataset_from_drive():\n",
        "    \"\"\"Copies essential Spider JSON files from Drive to local Colab storage.\"\"\"\n",
        "    drive_dataset_source_dir = f\"{DRIVE_BASE_DIR}/datasets/spider\"\n",
        "    print(f\"\\n--- Setting up Dataset ---\")\n",
        "    print(f\"Attempting to copy dataset JSON files from Google Drive path: {drive_dataset_source_dir}\")\n",
        "    drive_tables_path = f\"{drive_dataset_source_dir}/tables.json\"\n",
        "    drive_dev_path = f\"{drive_dataset_source_dir}/dev.json\"\n",
        "    dev_exists = os.path.exists(drive_dev_path)\n",
        "    tables_exists = os.path.exists(drive_tables_path)\n",
        "    if dev_exists and tables_exists:\n",
        "        print(\"Required JSON dataset files found. Copying to local Colab storage...\")\n",
        "        try:\n",
        "            os.makedirs(LOCAL_DATASET_DIR, exist_ok=True)\n",
        "            !cp -v \"{drive_dev_path}\" \"{LOCAL_DATASET_DIR}/\"\n",
        "            !cp -v \"{drive_tables_path}\" \"{LOCAL_DATASET_DIR}/\"\n",
        "            print(f\"Successfully copied JSON files to: {LOCAL_DATASET_DIR}\")\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL Error: Failed to copy JSON files from Google Drive: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        print(f\"CRITICAL Error: Essential dataset JSON files not found in Google Drive.\")\n",
        "        if not dev_exists: print(f\"  Missing: {drive_dev_path}\")\n",
        "        if not tables_exists: print(f\"  Missing: {drive_tables_path}\")\n",
        "        raise FileNotFoundError(\"Essential dataset JSON files not found in Google Drive.\")\n",
        "\n",
        "# --- Import schema utils and setup dataset ---\n",
        "try:\n",
        "    from schema_utils import load_tables_json, enhance_prompts_with_schema\n",
        "    print(\"Successfully imported from schema_utils.py\")\n",
        "except ImportError as e:\n",
        "     print(f\"Error: Could not import from schema_utils.py: {e}\")\n",
        "     raise\n",
        "\n",
        "# Setup dataset (copy dev.json and tables.json from Drive)\n",
        "setup_local_dataset_from_drive()\n",
        "\n",
        "# --- Main Evaluation Function ---\n",
        "def evaluate_model():\n",
        "    \"\"\"Loads model, data, generates predictions, calculates BLEU, prepares files for Spider eval.\"\"\"\n",
        "    print(f\"\\n--- Starting Evaluation ---\")\n",
        "    print(f\"Evaluating model: {MODEL_PATH}\")\n",
        "    print(f\"Using schema format: {SCHEMA_FORMAT} (with Types)\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1) Load database schemas from LOCAL storage\n",
        "    tables_path = os.path.join(LOCAL_DATASET_DIR, \"tables.json\")\n",
        "    try:\n",
        "        db_schemas = load_tables_json('tables.json')\n",
        "        print(f\"Loaded schemas for {len(db_schemas)} databases from {tables_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load schemas: {e}\")\n",
        "        raise\n",
        "\n",
        "    # 2) Load dev data from LOCAL storage\n",
        "    dev_path = os.path.join(LOCAL_DATASET_DIR, \"dev.json\")\n",
        "    try:\n",
        "        with open(dev_path, 'r', encoding='utf-8') as f:\n",
        "            dev_spider_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load dev data: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Apply limit if specified\n",
        "    original_dev_count = len(dev_spider_data)\n",
        "    if LIMIT > 0 and LIMIT < original_dev_count:\n",
        "        print(f\"Limiting evaluation to first {LIMIT} examples (out of {original_dev_count})\")\n",
        "        dev_spider_data = dev_spider_data[:LIMIT]\n",
        "\n",
        "    dev_df = pd.DataFrame(dev_spider_data)\n",
        "    print(f\"Loaded {len(dev_df)} development examples for evaluation.\")\n",
        "\n",
        "    # 3) Enhance prompts\n",
        "    try:\n",
        "        dev_t5_data = enhance_prompts_with_schema(dev_df, db_schemas, schema_format=SCHEMA_FORMAT)\n",
        "        if dev_t5_data.empty and len(dev_df) > 0: raise ValueError(\"Prompt enhancement failed.\")\n",
        "        eval_dataset = Dataset.from_pandas(dev_t5_data)\n",
        "    except Exception as e:\n",
        "         print(f\"Failed during prompt enhancement: {e}\")\n",
        "         raise\n",
        "\n",
        "    if len(eval_dataset) == 0:\n",
        "        print(\"\\nWarning: Evaluation dataset is empty after processing.\")\n",
        "        return {\"error\": \"Evaluation dataset empty\", \"bleu\": None}\n",
        "    print(\"\\nSample prompt (with types):\")\n",
        "    print(eval_dataset[0][\"input_text\"][:600] + \"...\")\n",
        "\n",
        "    # --- Ensure Tokenizer Files Exist ---\n",
        "    print(f\"\\nChecking/Ensuring tokenizer files exist in: {MODEL_PATH}\")\n",
        "    tokenizer_config_path = os.path.join(MODEL_PATH, \"tokenizer_config.json\")\n",
        "    if not os.path.exists(tokenizer_config_path):\n",
        "          print(f\"Tokenizer config not found in {MODEL_PATH}. Saving base t5 tokenizer there...\")\n",
        "          try:\n",
        "              exp_name_only = os.path.basename(MODEL_PATH)\n",
        "              exp_parts = exp_name_only.split('_')\n",
        "\n",
        "              model_size_in_path = \"large\"\n",
        "              found_size = False\n",
        "              for part in exp_parts:\n",
        "                  if part in [\"small\", \"base\", \"large\"]:\n",
        "                      model_size_in_path = part\n",
        "                      found_size = True\n",
        "                      print(f\"Determined model size '{model_size_in_path}' from experiment name.\")\n",
        "                      break\n",
        "\n",
        "              if not found_size:\n",
        "                  print(f\"Warning: Could not reliably determine model size from experiment name '{exp_name_only}'. Assuming default '{model_size_in_path}'.\")\n",
        "\n",
        "              base_model_name = f\"t5-{model_size_in_path}\"\n",
        "\n",
        "              print(f\"Loading base tokenizer '{base_model_name}' to save...\")\n",
        "              base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "              # Saving tokenizer files to the main experiment directory\n",
        "              base_tokenizer.save_pretrained(MODEL_PATH)\n",
        "              print(f\"Base tokenizer saved to {MODEL_PATH}.\")\n",
        "              # Add a small delay for Drive sync if saving just happened\n",
        "              print(\"Pausing briefly for Drive sync...\")\n",
        "              time.sleep(10)\n",
        "          except Exception as e:\n",
        "              print(f\"CRITICAL Error: Failed to load/save base tokenizer to {MODEL_PATH}: {e}\")\n",
        "              print(\"Cannot proceed without tokenizer files.\")\n",
        "              raise\n",
        "    else:\n",
        "          print(\"Tokenizer files presumed to exist.\")\n",
        "\n",
        "    # 4) Load Model & Tokenizer (Now should find tokenizer files)\n",
        "    print(f\"\\nLoading model and tokenizer from: {MODEL_PATH}\")\n",
        "    if not os.path.isdir(MODEL_PATH): raise FileNotFoundError(f\"Model directory not found: {MODEL_PATH}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
        "        print(\"Model and tokenizer loaded successfully.\")\n",
        "    except OSError as e:\n",
        "         # Specific check for missing model weights file error\n",
        "         if \"pytorch_model.bin\" in str(e) or \"model.safetensors\" in str(e):\n",
        "              print(f\"\\nCRITICAL Error: Model weight file (e.g., pytorch_model.bin or model.safetensors) not found in {MODEL_PATH}.\")\n",
        "              print(\"This likely means the training save was incomplete.\")\n",
        "              print(\"Please verify the contents of the directory in Google Drive.\")\n",
        "              print(\"If checkpoint folders exist, modify MODEL_PATH to point to the latest checkpoint.\")\n",
        "         else:\n",
        "              print(f\"Failed to load model/tokenizer from {MODEL_PATH}: {e}\")\n",
        "         raise\n",
        "    except Exception as e:\n",
        "         print(f\"Failed to load model/tokenizer from {MODEL_PATH}: {e}\")\n",
        "         raise\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        model.to(device)\n",
        "    except Exception as e:\n",
        "         print(f\"Warning: Failed to move model to {device}. Using CPU. Error: {e}\")\n",
        "         device = torch.device(\"cpu\")\n",
        "         model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 5) Generate Predictions\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    predictions = []\n",
        "    references = []\n",
        "    max_input_length = 1024 if SCHEMA_FORMAT == \"sql\" else 512\n",
        "    generation_max_length = 256\n",
        "\n",
        "    for example in tqdm(eval_dataset, desc=\"Generating SQL\"):\n",
        "        inputs = tokenizer(\n",
        "            example[\"input_text\"], return_tensors=\"pt\", truncation=True,\n",
        "            max_length=max_input_length, padding=False\n",
        "        ).to(device)\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                output_ids = model.generate(\n",
        "                    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
        "                    max_length=generation_max_length, num_beams=NUM_BEAMS, early_stopping=True,\n",
        "                )[0]\n",
        "            pred_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "            predictions.append(pred_text)\n",
        "            references.append(example[\"output_text\"])\n",
        "        except Exception as e:\n",
        "             print(f\"\\nError during generation: {e}\")\n",
        "             predictions.append(\"GENERATION_ERROR\")\n",
        "             references.append(example[\"output_text\"])\n",
        "\n",
        "    # 6) Compute BLEU\n",
        "    print(\"\\nComputing BLEU score...\")\n",
        "    bleu_score = None\n",
        "    try:\n",
        "        if predictions and references:\n",
        "             bleu_metric = evaluate.load(\"bleu\")\n",
        "             valid_preds = [p for p in predictions if p != \"GENERATION_ERROR\"]\n",
        "             valid_refs = [[r] for i, r in enumerate(references) if predictions[i] != \"GENERATION_ERROR\"]\n",
        "             if valid_preds and valid_refs:\n",
        "                  results = bleu_metric.compute(predictions=valid_preds, references=valid_refs)\n",
        "                  bleu_score = results['bleu']\n",
        "                  print(f\"Dev BLEU (on {len(valid_preds)} valid examples): {bleu_score:.4f}\")\n",
        "             else: print(\"No valid predictions/references for BLEU.\")\n",
        "        else: print(\"Predictions/references empty, skipping BLEU.\")\n",
        "    except Exception as e: print(f\"Failed to compute BLEU: {e}\")\n",
        "    bleu_file_path = os.path.join(OUTPUT_DIR, \"bleu_score.txt\")\n",
        "    try:\n",
        "        with open(bleu_file_path, \"w\") as f: f.write(f\"{bleu_score if bleu_score is not None else 'N/A'}\")\n",
        "        print(f\"BLEU score saved to: {bleu_file_path}\")\n",
        "    except Exception as e: print(f\"Error saving BLEU score: {e}\")\n",
        "\n",
        "\n",
        "    # 7) Save Predictions\n",
        "    print(\"\\nSaving predictions (JSON)...\")\n",
        "    predictions_list = []\n",
        "    for i in range(len(predictions)):\n",
        "        if i < len(dev_df):\n",
        "             predictions_list.append({\n",
        "                 \"question\": dev_df.iloc[i][\"question\"],\n",
        "                 \"gold_sql\": references[i],\n",
        "                 \"pred_sql\": predictions[i],\n",
        "                 \"db_id\": dev_df.iloc[i][\"db_id\"]\n",
        "             })\n",
        "    predictions_file = os.path.join(OUTPUT_DIR, \"predictions.json\")\n",
        "    try:\n",
        "        with open(predictions_file, \"w\", encoding=\"utf-8\") as f: json.dump(predictions_list, f, indent=2)\n",
        "        print(f\"Predictions saved to: {predictions_file}\")\n",
        "    except Exception as e: print(f\"Error saving predictions JSON: {e}\")\n",
        "\n",
        "\n",
        "    # 8) Prepare Files for Spider Evaluation Format\n",
        "    print(\"\\nPreparing files for local Spider evaluation...\")\n",
        "    gold_file = os.path.join(OUTPUT_DIR, \"gold_sql.txt\")\n",
        "    pred_file = os.path.join(OUTPUT_DIR, \"pred_sql.txt\")\n",
        "    try:\n",
        "        with open(gold_file, \"w\", encoding=\"utf-8\") as gold_f, \\\n",
        "             open(pred_file, \"w\", encoding=\"utf-8\") as pred_f:\n",
        "            for item in predictions_list:\n",
        "                clean_gold = str(item[\"gold_sql\"]).replace('\\t', ' ').replace('\\n', ' ')\n",
        "                clean_pred = str(item[\"pred_sql\"]).replace('\\t', ' ').replace('\\n', ' ')\n",
        "                gold_f.write(clean_gold + \"\\t\" + item[\"db_id\"] + \"\\n\")\n",
        "                pred_f.write(clean_pred + \"\\t\" + item[\"db_id\"] + \"\\n\")\n",
        "        print(f\"Spider evaluation files prepared in {OUTPUT_DIR}:\\n  - {os.path.basename(gold_file)}\\n  - {os.path.basename(pred_file)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing Spider format files: {e}\")\n",
        "\n",
        "\n",
        "    # 9) Run Spider evaluation\n",
        "    print(f\"\\n-> Skipping automatic Spider evaluation step in Colab.\")\n",
        "    print(f\"-> To get EX/EM scores, run evaluation.py locally using the files generated in:\")\n",
        "    print(f\"   {OUTPUT_DIR}\")\n",
        "    print(f\"   Required files: {os.path.basename(gold_file)}, {os.path.basename(pred_file)}\")\n",
        "    print(f\"   You will also need locally: evaluation.py, tables.json, database directory\")\n",
        "\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    minutes, seconds = divmod(elapsed_time, 60)\n",
        "    print(f\"\\n--- Colab evaluation script finished in {int(minutes)}m {int(seconds)}s ---\")\n",
        "\n",
        "    return {\n",
        "        \"model_path\": MODEL_PATH,\n",
        "        \"schema_format\": SCHEMA_FORMAT,\n",
        "        \"bleu\": bleu_score,\n",
        "        \"predictions_file\": predictions_file,\n",
        "        \"gold_file\": gold_file,\n",
        "        \"pred_file\": pred_file\n",
        "    }\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        results = evaluate_model()\n",
        "        print(\"\\nColab Script Results Summary:\")\n",
        "        if results:\n",
        "             print(json.dumps(results, indent=2))\n",
        "        else:\n",
        "             print(\"Evaluation function did not return results.\")\n",
        "\n",
        "    except Exception as e:\n",
        "         print(f\"\\n--- An error occurred during evaluation: {e} ---\")\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGoV48-zLnTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
