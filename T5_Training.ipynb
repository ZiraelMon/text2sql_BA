{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPittdRfZru+KrA94NUQkM3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8muKPChjTH2x"
      },
      "outputs": [],
      "source": [
        "# T5 Text-to-SQL Fine-Tuning Script\n",
        "# For bachelor thesis on schema-enhanced Text-to-SQL generation\n",
        "# VERSION: Disabled fp16 to debug NaN loss\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU device: {gpu_name}\")\n",
        "    print(f\"Memory: {gpu_memory:.2f} GB\")\n",
        "    if 'A100' not in gpu_name and 'H100' not in gpu_name:\n",
        "         print(\"Warning: T5-Large is memory intensive. Ensure sufficient GPU RAM.\")\n",
        "\n",
        "# Show GPU info\n",
        "!nvidia-smi\n",
        "\n",
        "# Install required packages\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q datasets transformers evaluate tensorboard accelerate huggingface-hub pandas\n",
        "print(\"Packages installed.\")\n",
        "\n",
        "# --- Verify Installation ---\n",
        "print(\"\\nVerifying package versions...\")\n",
        "!pip show datasets transformers evaluate tensorboard accelerate huggingface-hub torch pandas\n",
        "print(\"-\" * 30)\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- Configuration ---\n",
        "EXPERIMENT_NAME = \"t5_large_sql_types_schema_v3_nofp16\"\n",
        "SCHEMA_FORMAT = \"sql\"\n",
        "MODEL_SIZE = \"large\"\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 2e-5\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "WEIGHT_DECAY = 0.01\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MAX_TARGET_LENGTH = 256\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "RESUME_FROM_CHECKPOINT = False\n",
        "\n",
        "# Google Drive paths\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/text2sql\"\n",
        "DRIVE_OUTPUT_DIR = f\"{DRIVE_BASE_DIR}/{EXPERIMENT_NAME}\"\n",
        "DRIVE_DATASET_SOURCE_DIR = f\"{DRIVE_BASE_DIR}/datasets/spider\"\n",
        "DRIVE_LOGS_DIR = f\"{DRIVE_BASE_DIR}/logs/{EXPERIMENT_NAME}\"\n",
        "\n",
        "# Local paths\n",
        "LOCAL_DATASET_DIR = \"/content/datasets/spider\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_LOGS_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"--- Running Experiment: {EXPERIMENT_NAME} ---\")\n",
        "print(f\"Model: t5-{MODEL_SIZE}\")\n",
        "print(f\"Schema Format: {SCHEMA_FORMAT} (with Types)\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Per Device Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
        "print(f\"Gradient Clipping: {MAX_GRAD_NORM}\")\n",
        "print(f\"Max Input Length: {MAX_INPUT_LENGTH}\")\n",
        "print(f\"Max Target Length: {MAX_TARGET_LENGTH}\")\n",
        "print(f\"Warmup Ratio: {WARMUP_RATIO}\")\n",
        "print(f\"FP16 Enabled: False\")\n",
        "\n",
        "# --- Schema Utilities ---\n",
        "def load_tables_json(tables_path):\n",
        "    \"\"\"Load the tables.json file containing schema information.\"\"\"\n",
        "    full_path = os.path.join(LOCAL_DATASET_DIR, tables_path)\n",
        "    print(f\"Loading tables.json from: {full_path}\")\n",
        "    try:\n",
        "        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "            tables_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: tables.json not found at {full_path}\")\n",
        "        raise\n",
        "    db_schemas = {db_info['db_id']: db_info for db_info in tables_data}\n",
        "    return db_schemas\n",
        "\n",
        "def get_sql_schema_string(db_id, db_schemas):\n",
        "    \"\"\"Create SQL schema string including types, PKs, FKs.\"\"\"\n",
        "    if db_id not in db_schemas: raise ValueError(f\"DB ID '{db_id}' not found\")\n",
        "    schema_info = db_schemas[db_id]\n",
        "    tables = schema_info['table_names_original']\n",
        "    columns = schema_info['column_names_original']\n",
        "    column_types = schema_info['column_types']\n",
        "    primary_keys = set(schema_info.get('primary_keys', []))\n",
        "    fk_dict = {}\n",
        "    if isinstance(schema_info.get('foreign_keys'), list):\n",
        "        for fk_pair in schema_info['foreign_keys']:\n",
        "             if isinstance(fk_pair, (list, tuple)) and len(fk_pair) == 2:\n",
        "                 col1_idx, col2_idx = fk_pair\n",
        "                 if isinstance(col1_idx, int) and isinstance(col2_idx, int): fk_dict[col1_idx] = col2_idx\n",
        "    table_defs = []\n",
        "    for i, table in enumerate(tables):\n",
        "        table_columns = []\n",
        "        for col_idx, (tab_idx, col_name) in enumerate(columns):\n",
        "            if tab_idx == i:\n",
        "                col_type = column_types[col_idx].upper()\n",
        "                col_info = f\"{col_name} ({col_type})\"\n",
        "                if col_idx in primary_keys: col_info += \" (PRIMARY KEY)\"\n",
        "                if col_idx in fk_dict:\n",
        "                    ref_col_idx = fk_dict[col_idx]\n",
        "                    if 0 <= ref_col_idx < len(columns):\n",
        "                         ref_tab_idx, ref_col_name = columns[ref_col_idx]\n",
        "                         if 0 <= ref_tab_idx < len(tables):\n",
        "                              ref_table = tables[ref_tab_idx]\n",
        "                              col_info += f\" (FOREIGN KEY -> {ref_table}.{ref_col_name})\"\n",
        "                table_columns.append(col_info)\n",
        "        if table_columns:\n",
        "            table_columns.sort()\n",
        "            table_def = f\"Table: {table}\\\\nColumns: {', '.join(table_columns)}\"\n",
        "            table_defs.append(table_def)\n",
        "    table_defs.sort()\n",
        "    return \"\\\\n\".join(table_defs)\n",
        "\n",
        "def enhance_prompts_with_schema(data_df, db_schemas, schema_format=\"sql\"):\n",
        "    \"\"\"Enhance input prompts with schema information.\"\"\"\n",
        "    enhanced_rows = []\n",
        "    skipped_count = 0\n",
        "    total_count = len(data_df)\n",
        "    print_interval = max(1, total_count // 10)\n",
        "    print(f\"Enhancing {total_count} prompts...\")\n",
        "    for index, example in data_df.iterrows():\n",
        "        if index > 0 and index % print_interval == 0: print(f\"  Processed {index}/{total_count} examples...\")\n",
        "        db_id = example['db_id']\n",
        "        try:\n",
        "            if schema_format == \"sql\":\n",
        "                schema_str = get_sql_schema_string(db_id, db_schemas) # Uses func with types\n",
        "                input_text = f\"translate English to SQL: {example['question']} | database: {db_id} | schema:\\\\n{schema_str}\"\n",
        "            else: input_text = f\"translate English to SQL: {example['question']} | database: {db_id}\"\n",
        "            output_text = example['query']\n",
        "            enhanced_rows.append({\"input_text\": input_text, \"output_text\": output_text})\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Skipping example for db_id '{db_id}': {e}\")\n",
        "             skipped_count += 1\n",
        "    print(f\"  Processed {total_count}/{total_count} examples...\")\n",
        "    if skipped_count > 0: print(f\"Skipped {skipped_count} examples.\")\n",
        "    if not enhanced_rows: print(\"Warning: No rows enhanced.\")\n",
        "    return pd.DataFrame(enhanced_rows)\n",
        "\n",
        "# --- Setup Local Dataset from Drive ---\n",
        "def setup_local_dataset_from_drive():\n",
        "    \"\"\"Copy the Spider dataset JSON files from Google Drive to local storage.\"\"\"\n",
        "    print(f\"\\n--- Setting up Dataset ---\")\n",
        "    print(f\"Copying dataset from Google Drive path: {DRIVE_DATASET_SOURCE_DIR}\")\n",
        "    drive_tables_path = f\"{DRIVE_DATASET_SOURCE_DIR}/tables.json\"\n",
        "    drive_train_path = f\"{DRIVE_DATASET_SOURCE_DIR}/train_spider.json\"\n",
        "    drive_dev_path = f\"{DRIVE_DATASET_SOURCE_DIR}/dev.json\"\n",
        "    all_paths_exist = all(os.path.exists(p) for p in [drive_tables_path, drive_train_path, drive_dev_path])\n",
        "    if all_paths_exist:\n",
        "        print(\"Copying dataset files to local Colab storage...\")\n",
        "        try:\n",
        "            !cp -v \"{drive_tables_path}\" \"{LOCAL_DATASET_DIR}/\"\n",
        "            !cp -v \"{drive_train_path}\" \"{LOCAL_DATASET_DIR}/\"\n",
        "            !cp -v \"{drive_dev_path}\" \"{LOCAL_DATASET_DIR}/\"\n",
        "            print(\"Dataset files copied successfully.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error during file copy: {e}\")\n",
        "             raise\n",
        "    else:\n",
        "        missing = [p for p in [drive_tables_path, drive_train_path, drive_dev_path] if not os.path.exists(p)]\n",
        "        raise FileNotFoundError(f\"Missing files in Drive: {missing}\")\n",
        "\n",
        "# --- Run Setup Function ---\n",
        "setup_local_dataset_from_drive()\n",
        "\n",
        "# --- Load Database Schemas ---\n",
        "print(\"\\nLoading database schemas...\")\n",
        "db_schemas = load_tables_json('tables.json')\n",
        "print(f\"Loaded schemas for {len(db_schemas)} databases.\")\n",
        "\n",
        "# --- Load and Prepare Datasets ---\n",
        "def load_and_prepare_data(file_path, db_schemas_dict, schema_fmt):\n",
        "    \"\"\"Load, enhance and prepare dataset.\"\"\"\n",
        "    actual_file_path = os.path.join(LOCAL_DATASET_DIR, file_path)\n",
        "    print(f\"Loading data from: {actual_file_path}\")\n",
        "    try:\n",
        "        with open(actual_file_path, 'r', encoding='utf-8') as f:\n",
        "            spider_data = json.load(f)\n",
        "        df = pd.DataFrame(spider_data)\n",
        "        t5_data_df = enhance_prompts_with_schema(df, db_schemas_dict, schema_format=schema_fmt)\n",
        "        if t5_data_df is None or t5_data_df.empty: return None\n",
        "        dataset = Dataset.from_pandas(t5_data_df)\n",
        "        print(f\"Prepared {len(dataset)} examples from {actual_file_path}.\")\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data from {actual_file_path}: {e}\")\n",
        "        raise\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = load_and_prepare_data('train_spider.json', db_schemas, SCHEMA_FORMAT)\n",
        "dev_dataset = load_and_prepare_data('dev.json', db_schemas, SCHEMA_FORMAT)\n",
        "if train_dataset is None or dev_dataset is None:\n",
        "     raise ValueError(\"Failed to load train or dev dataset.\")\n",
        "\n",
        "# Log some examples to verify prompts\n",
        "print(\"\\nSample Prompts (with types):\")\n",
        "for i in range(min(2, len(train_dataset))):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Input length: {len(train_dataset[i]['input_text'])} characters\")\n",
        "    print(f\"Input: {train_dataset[i]['input_text'][:600]}...\") # Show more\n",
        "    print(f\"Output: {train_dataset[i]['output_text']}\")\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "model_name = f\"t5-{MODEL_SIZE}\"\n",
        "print(f\"\\nLoading model: {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    return_dict=True\n",
        ")\n",
        "print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "# --- Tokenization Function ---\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenizes input and target text.\"\"\"\n",
        "    input_texts = [text if text is not None else \"\" for text in examples['input_text']]\n",
        "    output_texts = [text if text is not None else \"\" for text in examples['output_text']]\n",
        "    model_inputs = tokenizer(\n",
        "        input_texts, max_length=MAX_INPUT_LENGTH, truncation=True,\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            output_texts, max_length=MAX_TARGET_LENGTH, truncation=True,\n",
        "        )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# --- Tokenize the Datasets ---\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=train_dataset.column_names, desc=\"Tokenizing training dataset\"\n",
        ")\n",
        "tokenized_dev = dev_dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=dev_dataset.column_names, desc=\"Tokenizing development dataset\"\n",
        ")\n",
        "print(f\"Training dataset tokenized: {len(tokenized_train)} examples\")\n",
        "print(f\"Development dataset tokenized: {len(tokenized_dev)} examples\")\n",
        "\n",
        "# --- Data Collator ---\n",
        "print(\"\\nData collator initializing...\")\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    padding=\"longest\",\n",
        "    pad_to_multiple_of=None\n",
        ")\n",
        "print(f\"Data collator using label_pad_token_id: {data_collator.label_pad_token_id}\")\n",
        "\n",
        "# --- Training Arguments ---\n",
        "print(\"\\nConfiguring training arguments...\")\n",
        "total_steps = len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "print(f\"Total training steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=DRIVE_OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_steps=warmup_steps,\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    logging_dir=DRIVE_LOGS_DIR,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    optim=\"adamw_torch\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# --- Trainer Initialization ---\n",
        "print(\"\\nInitializing Trainer...\")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "print(\"Trainer initialized.\")\n",
        "\n",
        "# --- Main Training Execution ---\n",
        "print(f\"\\n--- Starting Training ---\")\n",
        "start_time = time.time()\n",
        "checkpoint = None\n",
        "if RESUME_FROM_CHECKPOINT:\n",
        "    if os.path.isdir(DRIVE_OUTPUT_DIR):\n",
        "        checkpoints = [os.path.join(DRIVE_OUTPUT_DIR, d) for d in os.listdir(DRIVE_OUTPUT_DIR) if d.startswith('checkpoint-') and os.path.isdir(os.path.join(DRIVE_OUTPUT_DIR, d))]\n",
        "        if checkpoints:\n",
        "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
        "            print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
        "            checkpoint = latest_checkpoint\n",
        "        else: print(f\"No checkpoint found in {DRIVE_OUTPUT_DIR}.\")\n",
        "    else: print(f\"Output directory {DRIVE_OUTPUT_DIR} does not exist.\")\n",
        "\n",
        "try:\n",
        "    print(\"Running initial evaluation to check for NaN...\")\n",
        "    initial_eval = trainer.evaluate()\n",
        "    print(f\"Initial evaluation result: {initial_eval}\")\n",
        "    if 'eval_loss' in initial_eval and np.isnan(initial_eval['eval_loss']):\n",
        "        print(\"ERROR: NaN detected in evaluation loss even with fp16=False. Check data or learning rate.\")\n",
        "        print(\"Proceeding with training despite initial NaN...\")\n",
        "\n",
        "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n",
        "    # --- Post-Training Actions ---\n",
        "    print(\"\\n--- Training Finished ---\")\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "    print(\"\\nSaving the final model...\")\n",
        "    trainer.save_model()\n",
        "    trainer.save_state()\n",
        "    tokenizer.save_pretrained(training_args.output_dir)\n",
        "    print(f\"Model saved to {training_args.output_dir}\")\n",
        "\n",
        "    # Generate training report\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    report_path = os.path.join(training_args.output_dir, \"training_summary.txt\")\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(f\"Experiment: {EXPERIMENT_NAME}\\\\n\")\n",
        "        f.write(f\"Model: t5-{MODEL_SIZE}\\\\n\")\n",
        "        f.write(f\"Schema Format: {SCHEMA_FORMAT} (with Types)\\\\n\")\n",
        "        f.write(f\"FP16 Enabled: {training_args.fp16}\\\\n\")\n",
        "        f.write(f\"Epochs Configured: {EPOCHS}\\\\n\")\n",
        "        f.write(f\"Epochs Trained: {metrics.get('epoch', 0.0):.2f}\\\\n\")\n",
        "        f.write(f\"Training Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\\\\n\")\n",
        "        f.write(f\"Learning Rate: {LEARNING_RATE}\\\\n\")\n",
        "        f.write(f\"Batch Size (per device): {BATCH_SIZE}\\\\n\")\n",
        "        f.write(f\"Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}\\\\n\")\n",
        "        f.write(f\"Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\\\\n\")\n",
        "        f.write(f\"Weight Decay: {WEIGHT_DECAY}\\\\n\")\n",
        "        f.write(f\"Warmup Ratio: {WARMUP_RATIO} (Steps: {warmup_steps})\\\\n\")\n",
        "        f.write(f\"Max Grad Norm: {MAX_GRAD_NORM}\\\\n\")\n",
        "        f.write(f\"Max Input Length: {MAX_INPUT_LENGTH}\\\\n\")\n",
        "        f.write(f\"Max Target Length: {MAX_TARGET_LENGTH}\\\\n\")\n",
        "        f.write(\"\\\\nTraining Metrics:\\\\n\")\n",
        "        for key, value in metrics.items():\n",
        "            f.write(f\"  {key}: {value}\\\\n\")\n",
        "        f.write(f\"\\\\nModel saved to: {training_args.output_dir}\\\\n\")\n",
        "        f.write(f\"To evaluate the model, use the evaluation script.\\\\n\")\n",
        "    print(f\"Training summary saved to {report_path}\")\n",
        "    print(f\"Training completed in {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\\\nTraining interrupted by user. Saving current state...\")\n",
        "    interrupted_path = os.path.join(training_args.output_dir, \"interrupted_checkpoint\")\n",
        "    if 'trainer' in locals() and hasattr(trainer, 'save_model'): trainer.save_model(interrupted_path)\n",
        "    if 'tokenizer' in locals() and hasattr(tokenizer, 'save_pretrained'): tokenizer.save_pretrained(interrupted_path)\n",
        "    print(f\"Interrupted checkpoint potentially saved to {interrupted_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\\\n--- An error occurred during training: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\\\n--- Script Finished ---\")\n"
      ]
    }
  ]
}